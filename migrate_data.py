"""
NEXUS PRIME æ•°æ®è¿ç§»å·¥å…·
æ”¯æŒä» SQLite å¯¼å‡ºæ•°æ®åˆ° CSVï¼Œä»¥åŠä» CSV å¯¼å…¥åˆ° PostgreSQL
"""
import os
import sqlite3
import csv
import io
import click
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# --- é…ç½® ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SQLITE_DB_PATH = os.environ.get('SQLITE_DB_PATH') or os.path.join(BASE_DIR, 'instance', 'nexus_prime.db')
POSTGRES_DB_URL = os.environ.get('DATABASE_URL')
EXPORT_DIR = os.path.join(BASE_DIR, 'data_export')

# è¡¨å¯¼å…¥é¡ºåºï¼ˆæŒ‰ä¾èµ–å…³ç³»æ’åºï¼Œè¢«ä¾èµ–çš„è¡¨å…ˆå¯¼å…¥ï¼‰
TABLE_IMPORT_ORDER = [
    # 1. åŸºç¡€è¡¨ï¼ˆæ— å¤–é”®ä¾èµ–ï¼‰
    'alembic_version',
    'auth_roles',
    'auth_departments',
    'auth_permissions',
    'biz_categories',
    'biz_tags',
    'stock_warehouses',
    
    # 2. ç”¨æˆ·è¡¨ï¼ˆä¾èµ–è§’è‰²å’Œéƒ¨é—¨ï¼‰
    'auth_users',
    
    # 3. ä¸šåŠ¡ä¼™ä¼´è¡¨
    'biz_partners',
    
    # 4. äº§å“è¡¨ï¼ˆä¾èµ–åˆ†ç±»å’Œä¾›åº”å•†ï¼‰
    'biz_products',
    
    # 5. å¤šå¯¹å¤šå…³è”è¡¨
    'biz_product_tags',
    'roles_permissions',
    
    # 6. åº“å­˜ç›¸å…³
    'stock_quantities',
    'stock_logs',
    'stock_alerts',
    'stock_replenishment_suggestions',
    
    # 7. äº¤æ˜“ç›¸å…³
    'trade_orders',
    'trade_order_items',
    
    # 8. é‡‡è´­ç›¸å…³
    'purchase_orders',
    'purchase_order_items',
    'purchase_price_history',
    'supplier_performance',
    
    # 9. è´¢åŠ¡ç›¸å…³
    'finance_customer_credit',
    'finance_receivables',
    'finance_payments',
    'finance_statements',
    
    # 10. ç›˜ç‚¹ç›¸å…³
    'stock_takes',
    'stock_take_items',
    'stock_take_history',
    
    # 11. å†…å®¹ç®¡ç†
    'cms_articles',
    'cms_attachments',
    
    # 12. ç³»ç»Ÿæ—¥å¿—å’Œé€šçŸ¥
    'sys_audit_logs',
    'sys_notifications',
    'sys_ai_logs',
    'sys_ai_sessions',
    'sys_ai_messages',
    
    # 13. æŠ¥è¡¨ç›¸å…³
    'report_subscriptions',
    'generated_reports',
]


def get_sqlite_tables():
    """è·å– SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
    if not os.path.exists(SQLITE_DB_PATH):
        print(f"âŒ SQLite æ•°æ®åº“ä¸å­˜åœ¨: {SQLITE_DB_PATH}")
        return []
    
    conn = sqlite3.connect(SQLITE_DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
    tables = [row[0] for row in cursor.fetchall()]
    conn.close()
    return tables


def export_sqlite_to_csv():
    """ä» SQLite å¯¼å‡ºæ‰€æœ‰è¡¨åˆ° CSV"""
    if not os.path.exists(SQLITE_DB_PATH):
        print(f"âŒ SQLite æ•°æ®åº“ä¸å­˜åœ¨: {SQLITE_DB_PATH}")
        return
    
    # åˆ›å»ºå¯¼å‡ºç›®å½•
    if not os.path.exists(EXPORT_DIR):
        os.makedirs(EXPORT_DIR)
    
    conn = sqlite3.connect(SQLITE_DB_PATH)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    # è·å–æ‰€æœ‰è¡¨
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
    tables = [row[0] for row in cursor.fetchall()]
    
    print(f"ğŸ“¦ å‘ç° {len(tables)} ä¸ªè¡¨ï¼Œå¼€å§‹å¯¼å‡º...")
    
    for table in tables:
        try:
            cursor.execute(f"SELECT * FROM {table}")
            rows = cursor.fetchall()
            
            if not rows:
                print(f"âšª è¡¨ '{table}' ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # è·å–åˆ—å
            columns = [description[0] for description in cursor.description]
            
            # å†™å…¥ CSV
            csv_path = os.path.join(EXPORT_DIR, f"{table}.csv")
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(columns)
                for row in rows:
                    writer.writerow(row)
            
            print(f"âœ… è¡¨ '{table}' å¯¼å‡ºæˆåŠŸ ({len(rows)} è¡Œ)")
        except Exception as e:
            print(f"âŒ è¡¨ '{table}' å¯¼å‡ºå¤±è´¥: {e}")
    
    conn.close()
    print(f"\nğŸ‰ å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {EXPORT_DIR}")


def import_data_to_postgres(truncate=False, only_tables=None):
    """ä» CSV å¯¼å…¥æ•°æ®åˆ° PostgreSQL"""
    try:
        import psycopg2
        from psycopg2 import sql
    except ImportError:
        print("âŒ é”™è¯¯ï¼šæœªå®‰è£… psycopg2ã€‚è¯·è¿è¡Œ `pip install psycopg2-binary`")
        return

    if not POSTGRES_DB_URL:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® DATABASE_URLã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ã€‚")
        return

    if not os.path.isdir(EXPORT_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°å¯¼å‡ºç›®å½• {EXPORT_DIR}")
        return

    # è·å– CSV æ–‡ä»¶åˆ—è¡¨
    csv_files = [f for f in os.listdir(EXPORT_DIR) if f.endswith('.csv')]
    if only_tables:
        csv_files = [f for f in csv_files if f.replace('.csv', '') in only_tables]
    
    if not csv_files:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¯¼å…¥çš„ CSV æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ”Œ æ­£åœ¨è¿æ¥ PostgreSQL... (å‡†å¤‡å¯¼å…¥ {len(csv_files)} ä¸ªè¡¨)")
    
    # ä¿®å¤ Railway PostgreSQL URL
    db_url = POSTGRES_DB_URL
    if db_url.startswith('postgres://'):
        db_url = db_url.replace('postgres://', 'postgresql://', 1)
    
    try:
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return

    # é¢„å¤„ç†ï¼šä¿®å¤æ•°æ®åº“ç»“æ„
    print("ğŸ”§ æ­£åœ¨é¢„æ£€å¹¶ä¿®å¤æ•°æ®åº“ç»“æ„...")
    structure_fixes = [
        ("ALTER TABLE auth_users ALTER COLUMN password_hash TYPE TEXT;", "auth_users.password_hash å·²æ‰©å®¹"),
        ("ALTER TABLE cms_articles ALTER COLUMN content TYPE TEXT;", "cms_articles.content å·²æ‰©å®¹"),
        ("ALTER TABLE sys_ai_logs ALTER COLUMN prompt TYPE TEXT;", "sys_ai_logs.prompt å·²æ‰©å®¹"),
        ("ALTER TABLE sys_ai_logs ALTER COLUMN response TYPE TEXT;", "sys_ai_logs.response å·²æ‰©å®¹"),
        ("ALTER TABLE sys_ai_messages ALTER COLUMN content TYPE TEXT;", "sys_ai_messages.content å·²æ‰©å®¹"),
        ("ALTER TABLE sys_audit_logs ALTER COLUMN details TYPE TEXT;", "sys_audit_logs.details å·²æ‰©å®¹"),
    ]
    
    for fix_sql, msg in structure_fixes:
        try:
            cur.execute(fix_sql)
            conn.commit()
            print(f"  âœ… {msg}")
        except Exception:
            conn.rollback()

    # è·å–æ•°æ®åº“å½“å‰çš„åˆ—ç»“æ„å’Œæ•°æ®ç±»å‹
    cur.execute("""
        SELECT table_name, column_name, data_type 
        FROM information_schema.columns 
        WHERE table_schema = 'public'
    """)
    db_schema = {}
    db_column_types = {}
    for t, c, dtype in cur.fetchall():
        db_schema.setdefault(t, set()).add(c)
        db_column_types[(t, c)] = dtype

    # å¼€å¯"æé€Ÿæ¨¡å¼"ï¼ˆæš‚æ—¶ç¦ç”¨å¤–é”®æ£€æŸ¥ï¼‰
    cur.execute("SET session_replication_role = 'replica';")
    conn.commit()

    success_count = 0
    failed_tables = []
    
    # æŒ‰é¡ºåºå¯¼å…¥
    ordered_files = []
    for table in TABLE_IMPORT_ORDER:
        fname = f"{table}.csv"
        if fname in csv_files:
            ordered_files.append(fname)
            csv_files.remove(fname)
    # æ·»åŠ å‰©ä½™çš„æ–‡ä»¶
    ordered_files.extend(csv_files)
    
    try:
        for fname in ordered_files:
            table_name = fname.replace('.csv', '')
            
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            if table_name not in db_schema:
                print(f"âšª è·³è¿‡ï¼šæ•°æ®åº“ä¸­ä¸å­˜åœ¨è¡¨ '{table_name}'")
                continue

            # è¯»å– CSV è¡¨å¤´
            file_path = os.path.join(EXPORT_DIR, fname)
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                try:
                    csv_headers = next(reader)
                except StopIteration:
                    print(f"âšª è·³è¿‡ï¼šæ–‡ä»¶ '{fname}' ä¸ºç©º")
                    continue
            
            # è®¡ç®—å…¬å…±åˆ—
            valid_cols = [c for c in csv_headers if c in db_schema[table_name]]
            
            if not valid_cols:
                print(f"âŒ é”™è¯¯ï¼šè¡¨ '{table_name}' æ²¡æœ‰åŒ¹é…çš„åˆ—ï¼Œæ— æ³•å¯¼å…¥ã€‚")
                failed_tables.append(table_name)
                continue

            # è·å–å¸ƒå°”ç±»å‹çš„åˆ—
            bool_cols = set()
            for col in valid_cols:
                dtype = db_column_types.get((table_name, col), '')
                if dtype == 'boolean':
                    bool_cols.add(col)

            # å‡†å¤‡å†…å­˜æ•°æ®æµï¼ˆæ¸…æ´—æ•°æ®ï¼‰
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=valid_cols, extrasaction='ignore')
            writer.writeheader()

            with open(file_path, 'r', encoding='utf-8') as f:
                dict_reader = csv.DictReader(f)
                row_count = 0
                for row in dict_reader:
                    # æ¸…æ´—æ•°æ®
                    cleaned_row = {}
                    for col in valid_cols:
                        val = row.get(col, '')
                        # å¤„ç†ç©ºå­—ç¬¦ä¸²ä¸º NULL
                        if val == '' or val is None:
                            cleaned_row[col] = ''
                        # åªå¯¹å¸ƒå°”ç±»å‹åˆ—è¿›è¡Œå¸ƒå°”å€¼è½¬æ¢
                        elif col in bool_cols:
                            if val in ('True', 'true', '1'):
                                cleaned_row[col] = 'true'
                            elif val in ('False', 'false', '0'):
                                cleaned_row[col] = 'false'
                            else:
                                cleaned_row[col] = val
                        else:
                            cleaned_row[col] = val
                    writer.writerow(cleaned_row)
                    row_count += 1
            
            output.seek(0)
            if row_count == 0:
                print(f"âšª è¡¨ '{table_name}' æ— æ•°æ®ã€‚")
                continue

            # æ¸…ç©ºæ—§æ•°æ® (å¦‚æœéœ€è¦)
            if truncate:
                try:
                    cur.execute(sql.SQL("TRUNCATE TABLE {} RESTART IDENTITY CASCADE").format(sql.Identifier(table_name)))
                    conn.commit()
                except Exception as e:
                    conn.rollback()
                    cur.execute("SET session_replication_role = 'replica';")

            # æ‰§è¡Œ COPY å¯¼å…¥
            try:
                columns_sql = sql.SQL(',').join(map(sql.Identifier, valid_cols))
                copy_sql = sql.SQL("COPY {} ({}) FROM STDIN WITH (FORMAT csv, HEADER true, NULL '')").format(
                    sql.Identifier(table_name), columns_sql
                )
                cur.copy_expert(copy_sql, output)
                
                # ä¿®å¤ ID åºåˆ—
                if 'id' in valid_cols:
                    try:
                        cur.execute(sql.SQL(
                            "SELECT setval(pg_get_serial_sequence(%s, 'id'), COALESCE((SELECT MAX(id) FROM {}), 1));"
                        ).format(sql.Identifier(table_name)), [table_name])
                    except:
                        pass

                conn.commit()
                print(f"âœ… è¡¨ '{table_name}' å¯¼å…¥æˆåŠŸ ({row_count} è¡Œ)")
                success_count += 1
            except Exception as e:
                conn.rollback()
                cur.execute("SET session_replication_role = 'replica';")
                print(f"âŒ è¡¨ '{table_name}' å¯¼å…¥å¤±è´¥: {e}")
                failed_tables.append(table_name)

    finally:
        # æ¢å¤å¤–é”®æ£€æŸ¥
        cur.execute("SET session_replication_role = 'origin';")
        conn.commit()
        conn.close()
        
        print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸã€‚æˆåŠŸå¯¼å…¥ {success_count} ä¸ªè¡¨ã€‚")
        if failed_tables:
            print(f"âš ï¸  å¤±è´¥çš„è¡¨: {', '.join(failed_tables)}")
        
        print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸã€‚æˆåŠŸå¯¼å…¥ {success_count} ä¸ªè¡¨ã€‚")
        if failed_tables:
            print(f"âš ï¸  å¤±è´¥çš„è¡¨: {', '.join(failed_tables)}")


def verify_import():
    """éªŒè¯å¯¼å…¥ç»“æœ"""
    try:
        import psycopg2
    except ImportError:
        print("âŒ é”™è¯¯ï¼šæœªå®‰è£… psycopg2")
        return

    if not POSTGRES_DB_URL:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® DATABASE_URL")
        return

    db_url = POSTGRES_DB_URL
    if db_url.startswith('postgres://'):
        db_url = db_url.replace('postgres://', 'postgresql://', 1)

    try:
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return

    print("ğŸ“Š PostgreSQL æ•°æ®åº“ç»Ÿè®¡:")
    print("-" * 50)
    
    # è·å–æ‰€æœ‰è¡¨åŠå…¶è¡Œæ•°
    cur.execute("""
        SELECT schemaname, relname, n_live_tup 
        FROM pg_stat_user_tables 
        WHERE schemaname = 'public'
        ORDER BY relname;
    """)
    
    total_rows = 0
    for schema, table, rows in cur.fetchall():
        print(f"  {table}: {rows} è¡Œ")
        total_rows += rows
    
    print("-" * 50)
    print(f"  æ€»è®¡: {total_rows} è¡Œ")
    
    conn.close()


# --- å‘½ä»¤è¡Œæ¥å£ ---
@click.group()
def cli():
    """NEXUS PRIME æ•°æ®è¿ç§»å·¥å…·"""
    pass


@cli.command()
def export():
    """ä» SQLite å¯¼å‡ºæ•°æ®åˆ° CSV"""
    export_sqlite_to_csv()


@cli.command('import')
@click.option('--truncate', is_flag=True, help='å¯¼å…¥å‰æ¸…ç©ºè¡¨')
@click.option('--only', multiple=True, help='ä»…å¯¼å…¥æŒ‡å®šè¡¨')
def import_cmd(truncate, only):
    """ä» CSV å¯¼å…¥æ•°æ®åˆ° PostgreSQL"""
    import_data_to_postgres(truncate, list(only) if only else None)


@cli.command()
def verify():
    """éªŒè¯ PostgreSQL æ•°æ®åº“çŠ¶æ€"""
    verify_import()


@cli.command()
def tables():
    """åˆ—å‡º SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
    tables = get_sqlite_tables()
    if tables:
        print(f"ğŸ“‹ SQLite æ•°æ®åº“ä¸­çš„è¡¨ ({len(tables)} ä¸ª):")
        for t in sorted(tables):
            print(f"  - {t}")
    else:
        print("âŒ æ— æ³•è·å–è¡¨åˆ—è¡¨")


@cli.command()
def compare():
    """å¯¹æ¯”æœ¬åœ° CSV å’Œäº‘ç«¯ PostgreSQL æ•°æ®"""
    try:
        import psycopg2
    except ImportError:
        print("âŒ é”™è¯¯ï¼šæœªå®‰è£… psycopg2")
        return

    if not POSTGRES_DB_URL:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® DATABASE_URL")
        return

    if not os.path.isdir(EXPORT_DIR):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°å¯¼å‡ºç›®å½• {EXPORT_DIR}")
        return

    db_url = POSTGRES_DB_URL
    if db_url.startswith('postgres://'):
        db_url = db_url.replace('postgres://', 'postgresql://', 1)

    try:
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return

    print("ğŸ“Š æœ¬åœ° CSV vs äº‘ç«¯ PostgreSQL æ•°æ®å¯¹æ¯”:")
    print("-" * 60)
    print(f"{'è¡¨å':<30} {'æœ¬åœ°CSV':<10} {'äº‘ç«¯PG':<10} {'çŠ¶æ€':<10}")
    print("-" * 60)

    # è·å–äº‘ç«¯è¡¨è¡Œæ•°
    cur.execute("""
        SELECT relname, n_live_tup 
        FROM pg_stat_user_tables 
        WHERE schemaname = 'public';
    """)
    pg_counts = {row[0]: row[1] for row in cur.fetchall()}

    # è·å–æœ¬åœ° CSV è¡Œæ•°
    csv_files = [f for f in os.listdir(EXPORT_DIR) if f.endswith('.csv')]
    
    missing_in_pg = []
    missing_in_csv = []
    mismatch = []
    
    for fname in sorted(csv_files):
        table_name = fname.replace('.csv', '')
        
        # è®¡ç®— CSV è¡Œæ•°
        csv_count = 0
        try:
            with open(os.path.join(EXPORT_DIR, fname), 'r', encoding='utf-8') as f:
                csv_count = sum(1 for _ in f) - 1  # å‡å»è¡¨å¤´
                if csv_count < 0:
                    csv_count = 0
        except:
            csv_count = -1
        
        pg_count = pg_counts.get(table_name, -1)
        
        if pg_count == -1:
            status = "âŒ è¡¨ä¸å­˜åœ¨"
            missing_in_pg.append(table_name)
        elif csv_count == pg_count:
            status = "âœ… ä¸€è‡´"
        elif csv_count > pg_count:
            status = f"âš ï¸ å·® {csv_count - pg_count}"
            mismatch.append((table_name, csv_count, pg_count))
        else:
            status = f"ğŸ“ˆ å¤š {pg_count - csv_count}"
        
        print(f"{table_name:<30} {csv_count:<10} {pg_count:<10} {status}")
    
    # æ£€æŸ¥äº‘ç«¯æœ‰ä½†æœ¬åœ°æ²¡æœ‰çš„è¡¨
    csv_tables = {f.replace('.csv', '') for f in csv_files}
    for pg_table in pg_counts:
        if pg_table not in csv_tables:
            missing_in_csv.append(pg_table)
    
    print("-" * 60)
    
    if mismatch:
        print(f"\nâš ï¸  æ•°æ®ä¸ä¸€è‡´çš„è¡¨ ({len(mismatch)} ä¸ª):")
        for t, local, remote in mismatch:
            print(f"  - {t}: æœ¬åœ° {local} è¡Œ, äº‘ç«¯ {remote} è¡Œ (å·® {local - remote} è¡Œ)")
        print("\nğŸ’¡ å»ºè®®: è¿è¡Œ `python migrate_data.py import --truncate` é‡æ–°å¯¼å…¥")
    
    if missing_in_pg:
        print(f"\nâŒ äº‘ç«¯ç¼ºå¤±çš„è¡¨ ({len(missing_in_pg)} ä¸ª):")
        for t in missing_in_pg:
            print(f"  - {t}")
        print("\nğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®åº“è¿ç§»æ˜¯å¦å®Œæˆ (flask db upgrade)")
    
    if missing_in_csv:
        print(f"\nâ„¹ï¸  äº‘ç«¯ç‹¬æœ‰çš„è¡¨ ({len(missing_in_csv)} ä¸ª):")
        for t in missing_in_csv:
            print(f"  - {t}")
    
    if not mismatch and not missing_in_pg:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®å·²åŒæ­¥ï¼")
    
    conn.close()


@cli.command()
@click.option('--table', '-t', required=True, help='è¦æ£€æŸ¥çš„è¡¨å')
def check_table(table):
    """æ£€æŸ¥æŒ‡å®šè¡¨çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        import psycopg2
    except ImportError:
        print("âŒ é”™è¯¯ï¼šæœªå®‰è£… psycopg2")
        return

    if not POSTGRES_DB_URL:
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® DATABASE_URL")
        return

    db_url = POSTGRES_DB_URL
    if db_url.startswith('postgres://'):
        db_url = db_url.replace('postgres://', 'postgresql://', 1)

    try:
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        return

    print(f"ğŸ“‹ è¡¨ '{table}' è¯¦ç»†ä¿¡æ¯:")
    print("-" * 50)
    
    # è·å–è¡¨ç»“æ„
    cur.execute("""
        SELECT column_name, data_type, is_nullable, column_default
        FROM information_schema.columns
        WHERE table_schema = 'public' AND table_name = %s
        ORDER BY ordinal_position;
    """, [table])
    
    columns = cur.fetchall()
    if not columns:
        print(f"âŒ è¡¨ '{table}' ä¸å­˜åœ¨")
        conn.close()
        return
    
    print("åˆ—ç»“æ„:")
    for col_name, data_type, nullable, default in columns:
        null_str = "NULL" if nullable == 'YES' else "NOT NULL"
        default_str = f" DEFAULT {default}" if default else ""
        print(f"  {col_name}: {data_type} {null_str}{default_str}")
    
    # è·å–è¡Œæ•°
    cur.execute(f"SELECT COUNT(*) FROM {table}")
    count = cur.fetchone()[0]
    print(f"\nè¡Œæ•°: {count}")
    
    # æ˜¾ç¤ºå‰5è¡Œæ•°æ®
    if count > 0:
        cur.execute(f"SELECT * FROM {table} LIMIT 5")
        rows = cur.fetchall()
        col_names = [desc[0] for desc in cur.description]
        
        print(f"\nå‰ {min(5, count)} è¡Œæ•°æ®:")
        print(f"  {col_names}")
        for row in rows:
            print(f"  {row}")
    
    conn.close()


if __name__ == '__main__':
    cli()
